{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2310e2-6d87-44ae-8c1e-7ae15fd4305a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "\n",
    "# DeltaTable for creating the delta tables\n",
    "# pydeeq for data quality checks\n",
    "\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, sum as _sum\n",
    "from delta.tables import DeltaTable\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "import os\n",
    "\n",
    "print(os.environ['SPARK_VERSION'])\n",
    "\n",
    "# Get job parameters from Databricks\n",
    "date_str = dbutils.widgets.get(\"arrival_date\")\n",
    "# date_str = \"2024-07-25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefcd55f-ffa4-4c40-ba50-402754f599d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define file paths based on date parameter\n",
    "# below path point towards the volume (which is integrated with the ADLS gen2 storage account)\n",
    "\n",
    "booking_data = f\"/Volumes/scd_catalog/customer_bookings/customer_bookings/bookings_dataset/bookings_{date_str}.csv\"\n",
    "customer_data = f\"/Volumes/scd_catalog/customer_bookings/customer_bookings/customer_dataset/customers_{date_str}.csv\"\n",
    "print(booking_data)\n",
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dfee0a9-2cf8-435b-834e-a8ff4e27e40f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## READING THE DATA FROM THE SOURCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cb779cb-b73b-4047-976e-855cc1d16d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read booking data\n",
    "booking_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .load(booking_data)\n",
    "\n",
    "booking_df.printSchema()\n",
    "display(booking_data)\n",
    "\n",
    "# Read customer data for scd2 merge\n",
    "customer_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .load(customer_data)\n",
    "\n",
    "customer_df.printSchema()\n",
    "display(customer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4957b9-2e81-44ee-bc33-e94bb068bcca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DATA QUALITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e407a76a-ea77-4cc3-ab93-d88bfae79a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Quality Checks on booking data\n",
    "# It's like defining the constraints for the data\n",
    "\n",
    "check_incremental = Check(spark, CheckLevel.Error, \"Booking Data Check\") \\\n",
    "    .hasSize(lambda x: x > 0) \\\n",
    "    .isUnique(\"booking_id\", hint=\"Booking ID is not unique throught\") \\\n",
    "    .isComplete(\"customer_id\") \\\n",
    "    .isComplete(\"amount\") \\\n",
    "    .isNonNegative(\"amount\") \\\n",
    "    .isNonNegative(\"quantity\") \\\n",
    "    .isNonNegative(\"discount\")\n",
    "\n",
    "# Data Quality Checks on customer data\n",
    "# check_scd = Check(spark, CheckLevel.Error, \"Customer Data Check\") \\\n",
    "#     .hasSize(lambda x: x > 0) \\\n",
    "#     .isUnique(\"customer_id\") \\\n",
    "#     .isComplete(\"customer_name\") \\\n",
    "#     .isComplete(\"customer_address\") \\\n",
    "#     .isComplete(\"phone_number\") \\\n",
    "#     .isComplete(\"email\")\n",
    "\n",
    "check_scd = Check(spark, CheckLevel.Error, \"Customer Data Check\") \\\n",
    "    .hasSize(lambda x: x > 0) \\\n",
    "    .isUnique(\"customer_id\") \\\n",
    "    .isComplete(\"customer_name\") \\\n",
    "    .isComplete(\"customer_address\") \\\n",
    "    .isComplete(\"email\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1469ad16-4475-4af6-a0bd-bf8e9f02a4cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here we are running the data quality checks, by choosing the df and the appropriate check which defined earlier\n",
    "# Run the verification suite\n",
    "booking_dq_check = VerificationSuite(spark) \\\n",
    "    .onData(booking_df) \\\n",
    "    .addCheck(check_incremental) \\\n",
    "    .run()\n",
    "\n",
    "customer_dq_check = VerificationSuite(spark) \\\n",
    "    .onData(customer_df) \\\n",
    "    .addCheck(check_scd) \\\n",
    "    .run()\n",
    "\n",
    "# In this code, we are converting the results of the data quality checks into a dataframe and displaying it\n",
    "# simplely displaying the results\n",
    "booking_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, booking_dq_check)\n",
    "display(booking_dq_check_df)\n",
    "\n",
    "customer_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, customer_dq_check)\n",
    "display(customer_dq_check_df)\n",
    "\n",
    "# Check if verification passed\n",
    "# If any of the checks failed, we will get an error message\n",
    "if booking_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data Quality Checks Failed for Booking Data\")\n",
    "\n",
    "if customer_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data Quality Checks Failed for Customer Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e999647-7d2d-48b4-83a9-7b9ea28c4f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## TRANSFORMATION ON THE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b1f17b6-6d4b-4887-a98e-8f7a0988c391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Doing some transformation on top of the two dataframes\n",
    "\n",
    "# Add ingestion timestamp to booking data\n",
    "booking_df_incremental = booking_df.withColumn(\"ingestion_time\", current_timestamp())\n",
    "\n",
    "# Join booking data with customer data\n",
    "df_joined = booking_df_incremental.join(customer_df, \"customer_id\")\n",
    "\n",
    "# Business transformation: calculate total cost after discount and filter\n",
    "df_transformed = df_joined \\\n",
    "    .withColumn(\"total_cost\", col(\"amount\") - col(\"discount\")) \\\n",
    "    .filter(col(\"quantity\") > 0)\n",
    "\n",
    "# Group by and aggregate df_transformed\n",
    "df_transformed_agg = df_transformed \\\n",
    "    .groupBy(\"booking_type\", \"customer_id\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_cost\").alias(\"total_amount_sum\"),\n",
    "        _sum(\"quantity\").alias(\"total_quantity_sum\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e83fa089-e79a-4be7-9d15-1d6f41a4f725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OPERATIONS ON FACT TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75df559e-614e-4c72-bc31-79b96dc1df8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the Delta table exists\n",
    "\n",
    "fact_table_path = \"scd_catalog.customer_bookings.fact_bookings\" # defining the file path\n",
    "fact_table_exists = spark._jsparkSession.catalog().tableExists(fact_table_path) # checking exists or not\n",
    "\n",
    "\n",
    "if fact_table_exists: # if true\n",
    "    # Read the existing fact table\n",
    "    df_existing_fact = spark.read.format(\"delta\").table(fact_table_path)  # reading the existing table\n",
    "    \n",
    "    # Combine the aggregated data\n",
    "    df_combined = df_existing_fact.unionByName(df_transformed_agg, allowMissingColumns=True)  # combining the data\n",
    "    \n",
    "    # Perform another group by and aggregation on the combined data\n",
    "    df_final_agg = df_combined \\\n",
    "        .groupBy(\"booking_type\", \"customer_id\") \\\n",
    "        .agg(\n",
    "            _sum(\"total_amount_sum\").alias(\"total_amount_sum\"),\n",
    "            _sum(\"total_quantity_sum\").alias(\"total_quantity_sum\")\n",
    "        )\n",
    "else: # if false\n",
    "    # If the fact table doesn't exist, use the aggregated transformed data directly\n",
    "    df_final_agg = df_transformed_agg\n",
    "\n",
    "display(df_final_agg)\n",
    "\n",
    "# Write the final aggregated data back to the Delta table\n",
    "df_final_agg.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(fact_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9ebf6f9-a990-41dc-960a-4c573cc55a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OPERATIONS ON DIM TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de0a7d41-27a0-4dde-b537-26741bf13cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scd_table_path = \"scd_catalog.customer_bookings.dim_customer\" # defining the scd table file path\n",
    "scd_table_exists = spark._jsparkSession.catalog().tableExists(scd_table_path) # checks table exists or not\n",
    "\n",
    "# Check if the customers table exists\n",
    "if scd_table_exists:\n",
    "    # Load the existing SCD table\n",
    "    scd_table = DeltaTable.forName(spark, scd_table_path)\n",
    "    display(scd_table.toDF())\n",
    "    \n",
    "    # Perform SCD2 merge logic\n",
    "    scd_table.alias(\"scd\") \\\n",
    "        .merge(\n",
    "            customer_df.alias(\"updates\"),\n",
    "            \"scd.customer_id = updates.customer_id and scd.valid_to = '9999-12-31'\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"valid_to\": \"updates.valid_from\",\n",
    "        }) \\\n",
    "        .execute()  # updates will undertaken here\n",
    "\n",
    "    customer_df.write.format(\"delta\").mode(\"append\").saveAsTable(scd_table_path)  # appending the data\n",
    "else:\n",
    "    # If the SCD table doesn't exist, write the customer data as a new Delta table\n",
    "    customer_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(scd_table_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bookings_SCD_2_Execution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
